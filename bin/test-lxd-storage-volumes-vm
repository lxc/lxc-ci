#!/bin/sh
set -eux

waitSnapdSeed() (
  set +x
  for i in $(seq 60); do # Wait up to 60s.
    if systemctl show snapd.seeded.service --value --property SubState | grep -qx exited; then
      return 0 # Success.
    fi

    sleep 1
  done

  echo "snapd not seeded after ${i}s"
  return 1 # Failed.
)

cleanup() {
    echo ""
    if [ "${FAIL}" = "1" ]; then
        echo "Test failed"
        exit 1
    fi

    echo "Test passed"
    exit 0
}

poolDriverList="${1:-dir btrfs lvm lvm-thin zfs ceph}"
FAIL=1
trap cleanup EXIT HUP INT TERM

# Wait for snapd seeding
waitSnapdSeed

# Configure to use the proxy
curl -s http://canonical-lxd.stgraber.org/config/snapd.sh | sh

# Configure for ceph use
curl -s http://canonical-lxd.stgraber.org/config/ceph.sh | sh

# Install LXD
while [ -e /usr/bin/lxd ]; do
    apt-get remove --purge --yes lxd lxd-client lxcfs liblxc1
done
apt-get remove --purge cloud-init --yes
apt-get install --no-install-recommends --yes genisoimage
snap remove lxd || true
snap install lxd --channel=latest/edge
lxd waitready --timeout=300

waitVMAgent() (
  set +x
  # shellcheck disable=SC3043
  local vmName="$1"
  for i in $(seq 90) # Wait up to 90s.
  do
    if lxc info "${vmName}" | grep -qF 127.0.0.1; then
      return 0 # Success.
    fi

    sleep 1
  done

  echo "VM ${vmName} agent not running after ${i}s"
  return 1 # Failed.
)

# Configure LXD
lxc project switch default
lxc network create lxdbr0
lxc project create test -c features.images=false
lxc project switch test
lxc profile device add default eth0 nic network=lxdbr0

poolName="vmpool$$"

for poolDriver in $poolDriverList
do
	echo "==> Create storage pool using driver ${poolDriver}"
	if [ "${poolDriver}" = "dir" ]; then
		lxc storage create "${poolName}" "${poolDriver}" volume.size=5GB
	elif [ "${poolDriver}" = "ceph" ]; then
		lxc storage create "${poolName}" "${poolDriver}" source="${poolName}" volume.size=5GB
        elif [ "${poolDriver}" = "lvm" ]; then
                lxc storage create "${poolName}" "${poolDriver}" size=40GiB lvm.use_thinpool=false volume.size=5GB
        elif [ "${poolDriver}" = "lvm-thin" ]; then
                lxc storage create "${poolName}" lvm size=20GiB volume.size=5GB
	else
		lxc storage create "${poolName}" "${poolDriver}" size=20GB volume.size=5GB
	fi

	echo "==> Create VM"
	lxc init images:ubuntu/20.04 v1 --vm -s "${poolName}"
	lxc init images:ubuntu/20.04 v2 --vm -s "${poolName}"

	echo "==> Create custom block volume and attach it to VM"
	lxc storage volume create "${poolName}" vol1 --type=block size=10MB
	lxc storage volume attach "${poolName}" vol1 v1

	echo "==> Create custom volume and attach it to VM"
	lxc storage volume create "${poolName}" vol4 size=10MB
	lxc storage volume attach "${poolName}" vol4 v1 /foo

	echo "==> Start VM and add content to custom block volume"
	lxc start v1
	waitVMAgent v1

	lxc exec v1 -- /bin/sh -c "mkfs.ext4 /dev/sdb && mount /dev/sdb /mnt && echo foo > /mnt/bar && umount /mnt"

	echo "==> Stop VM and detach custom volumes"
	lxc stop -f v1
	lxc storage volume detach "${poolName}" vol1 v1
	lxc storage volume detach "${poolName}" vol4 v1

	echo "==> Backup custom block volume"
	lxc storage volume export "${poolName}" vol1 vol1.tar.gz
	lxc storage volume export "${poolName}" vol1 vol1-optimized.tar.gz --optimized-storage

	echo "==> Import custom block volume"
	lxc storage volume import "${poolName}" vol1.tar.gz vol2
	lxc storage volume import "${poolName}" vol1-optimized.tar.gz vol3
	rm vol1.tar.gz
	rm vol1-optimized.tar.gz

	echo "==> Import custom ISO volume"
	tmp_iso_dir="$(mktemp -d)"
	echo foo > "${tmp_iso_dir}/foo"
	genisoimage -o vol5.iso "${tmp_iso_dir}"
	rm -f "${tmp_iso_dir}/foo"
	echo bar > "${tmp_iso_dir}/bar"
	genisoimage -o vol6.iso "${tmp_iso_dir}"
	rm -rf "${tmp_iso_dir}"
	lxc storage volume import "${poolName}" vol5.iso vol5
	lxc storage volume import "${poolName}" vol6.iso vol6
	rm -f vol5.iso vol6.iso

	echo "==> Attach custom block volumes to VM"
	# Both volumes can be attached at the same time. The VM will have /dev/sdb and /dev/sdc.
	# It doesn't matter which volume is which device as they contain the same content.
	lxc storage volume attach "${poolName}" vol2 v1
	lxc storage volume attach "${poolName}" vol3 v1

	echo "==> Attach custom ISO volumes to VM"
	lxc storage volume attach "${poolName}" vol5 v1
	lxc storage volume attach "${poolName}" vol6 v1
	lxc storage volume attach "${poolName}" vol5 v2
	lxc storage volume attach "${poolName}" vol6 v2

	echo "==> Start VM and check content"
	lxc start v1
	lxc start v2
	waitVMAgent v1
	waitVMAgent v2

	# shellcheck disable=2016
	lxc exec v1 -- /bin/sh -c 'mount /dev/sdb /mnt && [ $(cat /mnt/bar) = foo ] && umount /mnt'
	# shellcheck disable=2016
	lxc exec v1 -- /bin/sh -c 'mount /dev/sdc /mnt && [ $(cat /mnt/bar) = foo ] && umount /mnt'

	# mount ISOs and check content
	# shellcheck disable=2016
	lxc exec v1 -- /bin/sh -c 'mount /dev/sr0 /mnt && [ $(cat /mnt/foo) = foo ] && ! touch /mnt/foo && umount /mnt'
	# shellcheck disable=2016
	lxc exec v1 -- /bin/sh -c 'mount /dev/sr1 /mnt && [ $(cat /mnt/bar) = bar ] && ! touch /mnt/bar && umount /mnt'

	# concurrent readonly ISO mounts
	# shellcheck disable=2016
	lxc exec v1 -- /bin/sh -c 'mount /dev/sr0 /mnt && [ $(cat /mnt/foo) = foo ] && ! touch /mnt/foo'
	# shellcheck disable=2016
	lxc exec v2 -- /bin/sh -c 'mount /dev/sr0 /mnt && [ $(cat /mnt/foo) = foo ] && ! touch /mnt/foo'
	lxc exec v1 -- umount /mnt
	lxc exec v2 -- umount /mnt

	echo "==> Detaching volumes"
	lxc stop -f v1
	lxc delete -f v2
	lxc storage volume detach "${poolName}" vol2 v1
	lxc storage volume detach "${poolName}" vol3 v1
	lxc storage volume detach "${poolName}" vol6 v1

	echo "==> Publishing VM to image"
	lxc storage volume create "${poolName}" images --project=default
	lxc config set storage.images_volume "${poolName}"/images
	lxc publish v1 --alias v1image
	lxc launch v1image v2 -s "${poolName}"
	waitVMAgent v2
	lxc delete v2 -f
	lxc image delete v1image
	lxc config unset storage.images_volume
	lxc storage volume delete "${poolName}" images --project=default

	echo "==> Deleting VM"
	lxc rm v1

	echo "==> Deleting storage pool and volumes"
	lxc storage volume rm "${poolName}" vol1
	lxc storage volume rm "${poolName}" vol2
	lxc storage volume rm "${poolName}" vol3
	lxc storage volume rm "${poolName}" vol4
	lxc storage volume rm "${poolName}" vol5
	lxc storage volume rm "${poolName}" vol6
	lxc storage rm "${poolName}"
done

lxc profile device remove default eth0
lxc project switch default
lxc project delete test
lxc network delete lxdbr0

FAIL=0
