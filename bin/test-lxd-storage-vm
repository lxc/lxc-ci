#!/bin/sh
set -eux

cleanup() {
    echo ""
    if [ "${FAIL}" = "1" ]; then
        echo "Test failed"
        exit 1
    fi

    echo "Test passed"
    exit 0
}

poolDriverList="${1:-dir btrfs lvm zfs ceph}"
FAIL=1
trap cleanup EXIT HUP INT TERM

# Wait for snapd seeding
sleep 1m

# Configure to use the proxy
curl -s http://canonical-lxd.stgraber.org/config/snapd.sh | sh

# Configure for ceph use
curl -s http://canonical-lxd.stgraber.org/config/ceph.sh | sh

# Install LXD
while :; do
    [ ! -e /usr/bin/lxd ] && break
    apt remove --purge lxd lxd-client --yes && break
done
snap install lxd --edge
snap install jq
snap refresh lxd --channel=latest/edge
lxd waitready --timeout=300

# Configure LXD
lxc network create lxdbr0
lxc profile device add default eth0 nic network=lxdbr0

poolName="vmpool$$"

for poolDriver in $poolDriverList
do
        echo "==> Create storage pool using driver ${poolDriver}"
        if [ "${poolDriver}" = "dir" ]; then
                lxc storage create "${poolName}" "${poolDriver}"
        elif [ "${poolDriver}" = "ceph" ]; then
                lxc storage create "${poolName}" "${poolDriver}" source="${poolName}"
        else
                lxc storage create "${poolName}" "${poolDriver}" size=20GB
        fi

        echo "==> Create VM and boot"
        lxc init images:ubuntu/20.04/cloud v1 --vm -s "${poolName}"
        lxc start v1
        sleep 90
        lxc info v1

        echo "==> Check config drive is readonly"
        # Check 9p config drive share is exported readonly.
        lxc exec v1 -- mount -t 9p config /srv
        ! lxc exec v1 -- touch /srv/lxd-test || false
        lxc exec v1 -- umount /srv

        # Check virtiofs config drive share is exported readonly.
        lxc exec v1 -- mount -t virtiofs config /srv
        ! lxc exec v1 -- touch /srv/lxd-test || false
        lxc exec v1 -- umount /srv

        echo "==> Checking VM root disk size is 10GB"
        lxc exec v1 -- df -B1000000000 | grep sda2 | grep 10

        echo "==> Increasing VM root disk size for next boot"
        lxc config device set v1 root size=11GB
        lxc config get v1 volatile.root.apply_quota | grep true
        lxc stop -f v1
        lxc start v1
        sleep 90

        echo "==> Checking VM root disk size is 11GB"
        lxc exec v1 -- df -B1000000000 | grep sda2 | grep 11

        echo "==> Check VM shrink is blocked"
        ! lxc config device set v1 root size=10GB || false

        echo "==> Checking additional disk device support"
        lxc stop -f v1
        # Create directory with a file for directory disk tests.
        mkdir "/tmp/lxd-test-${poolName}"
        touch "/tmp/lxd-test-${poolName}/lxd-test"

        # Create empty block file for block disk tests.
        truncate -s 5m "/tmp/lxd-test-${poolName}/lxd-test-block"

        # Add disks
        lxc config device add v1 dir1rw disk source="/tmp/lxd-test-${poolName}" path="/srv/rw"
        lxc config device add v1 dir1ro disk source="/tmp/lxd-test-${poolName}" path="/srv/ro" readonly=true
        lxc config device add v1 block1ro disk source="/tmp/lxd-test-${poolName}/lxd-test-block" readonly=true
        lxc config device add v1 block1rw disk source="/tmp/lxd-test-${poolName}/lxd-test-block"
        lxc start v1
        sleep 90

        # Check there is only 1 mount for each directory disk and that it is mounted with the appropriate options.
        lxc exec v1 -- mount | grep '/srv/rw type' -c | grep 1
        lxc exec v1 -- mount | grep '/srv/ro type' -c | grep 1

        # RW disks should use virtio-fs when used with the snap.
        lxc exec v1 -- mount | grep 'lxd_dir1rw on /srv/rw type virtiofs (rw,relatime)'

        # RO disks should use virtio-fs when used with the snap but be mounted readonly.
        lxc exec v1 -- mount | grep 'lxd_dir1ro on /srv/ro type virtiofs (ro,relatime)'

        # Check UID/GID are correct.
        lxc exec v1 -- stat -c '%u:%g' /srv/rw | grep '0:0'
        lxc exec v1 -- stat -c '%u:%g' /srv/ro | grep '0:0'

        # Remount the readonly disk as rw inside VM and check that the disk is still readonly at the LXD layer.
        lxc exec v1 -- mount -oremount,rw /srv/ro
        lxc exec v1 -- mount | grep 'lxd_dir1ro on /srv/ro type virtiofs (rw,relatime)'
        ! lxc exec v1 -- touch /srv/ro/lxd-test-ro || false
        ! lxc exec v1 -- mkdir /srv/ro/lxd-test-ro || false
        ! lxc exec v1 -- rm /srv/ro/lxd-test.txt || false
        ! lxc exec v1 -- chmod 777 /srv/ro || false

        ## Mount the readonly disk as rw inside VM using 9p and check the disk is still readonly at the LXD layer.
        lxc exec v1 -- mkdir /srv/ro9p
        lxc exec v1 -- mount -t 9p lxd_dir1ro /srv/ro9p
        lxc exec v1 -- mount | grep 'lxd_dir1ro on /srv/ro9p type 9p (rw,relatime,sync,dirsync,access=client,trans=virtio)'
        ! lxc exec v1 -- touch /srv/ro9p/lxd-test-ro || false
        ! lxc exec v1 -- mkdir /srv/ro9p/lxd-test-ro || false
        ! lxc exec v1 -- rm /srv/ro9p/lxd-test.txt || false
        ! lxc exec v1 -- chmod 777 /srv/ro9p || false

        # Check writable disk is writable.
        lxc exec v1 -- touch /srv/rw/lxd-test-rw
        stat -c '%u:%g' "/tmp/lxd-test-${poolName}/lxd-test-rw" | grep "0:0"
        lxc exec v1 -- rm /srv/rw/lxd-test-rw
        lxc exec v1 -- rm /srv/rw/lxd-test

        # Check block disks are available.
        lxc exec v1 -- stat -c "%F" /dev/sdb | grep "block special file"
        lxc exec v1 -- stat -c "%F" /dev/sdc | grep "block special file"

        # Check the rw driver accepts writes and the ro does not.
        ! lxc exec v1 -- dd if=/dev/urandom of=/dev/sdb bs=512 count=2 || false
        lxc exec v1 -- dd if=/dev/urandom of=/dev/sdc bs=512 count=2

        # Remove temporary directory (should now be empty aside from block file).
        lxc stop -f v1
        rm "/tmp/lxd-test-${poolName}/lxd-test-block"
        rmdir "/tmp/lxd-test-${poolName}"

        echo "==> Deleting VM"
        lxc delete -f v1

        echo "==> Change volume.size on pool and create VM"
        lxc storage set "${poolName}" volume.size 6GB
        lxc init images:ubuntu/20.04/cloud v1 --vm -s "${poolName}"
        lxc start v1
        sleep 90
        lxc info v1

        echo "==> Checking VM root disk size is 6GB"
        lxc exec v1 -- df -B1000000000 | grep sda2 | grep 6

        echo "==> Deleting VM and reset pool volume.size"
        lxc delete -f v1
        lxc storage unset "${poolName}" volume.size

        if [ "${poolDriver}" = "lvm" ]; then
                echo "==> Change volume.block.filesystem on pool and create VM"
                lxc storage set "${poolName}" volume.block.filesystem xfs
                lxc init images:ubuntu/20.04/cloud v1 --vm -s "${poolName}"
                lxc start v1
                sleep 90
                lxc info v1

                echo "==> Checking VM config disk filesyste is XFS"
                serverPID="$(lxc query /1.0 | jq .environment.server_pid)"
                nsenter -m -t "${serverPID}" stat -f -c %T /var/snap/lxd/common/lxd/virtual-machines/v1 | grep xfs

                echo "==> Deleting VM"
                lxc delete -f v1
                lxc storage unset "${poolName}" volume.block.filesystem
        fi

        echo "==> Create VM from profile with small disk size"
        lxc profile copy default vmsmall
        lxc profile device add vmsmall root disk pool="${poolName}" path=/ size=7GB
        lxc init images:ubuntu/20.04/cloud v1 --vm -p vmsmall
        lxc start v1
        sleep 90
        lxc info v1

        echo "==> Checking VM root disk size is 7GB"
        lxc exec v1 -- df -B1000000000 | grep sda2 | grep 7
        lxc stop -f v1

        echo "==> Copy to different storage pool and check size"
        dstPoolDriver=zfs # Use ZFS storage pool as that fixed volumes not files.
        if [ "${poolDriver}" = "zfs" ]; then
                dstPoolDriver=lvm # Use something different when testing ZFS.
        fi

        lxc storage create "${poolName}2" "${dstPoolDriver}" size=20GB
        lxc copy v1 v2 -s "${poolName}2"
        lxc start v2
        sleep 90
        lxc info v2

        echo "==> Checking copied VM root disk size is 7GB"
        lxc exec v2 -- df -B1000000000 | grep sda2 | grep 7
        lxc delete -f v2

        echo "==> Grow above default volume size and copy to different storage pool"
        lxc config device override v1 root size=11GB
        lxc copy v1 v2 -s "${poolName}2"
        lxc start v2
        sleep 90
        lxc info v2

        echo "==> Checking copied VM root disk size is 11GB"
        lxc exec v2 -- df -B1000000000 | grep sda2 | grep 11
        lxc delete -f v2

        echo "==> Publishing larger VM"
        lxc start v1 # Start to ensure cloud-init grows filesystem before publish.
        sleep 90
        lxc info v1
        lxc stop -f v1
        lxc publish v1 --alias vmbig
        lxc delete -f v1
        lxc storage set "${poolName}" volume.size 9GB

        echo "==> Check VM create fails when image larger than volume.size"
        ! lxc init vmbig v1 --vm -s "${poolName}" || false

        echo "==> Check VM create succeeds when no volume.size set"
        lxc storage unset "${poolName}" volume.size
        lxc init vmbig v1 --vm -s "${poolName}"
        lxc start v1
        sleep 90
        lxc info v1

        echo "==> Checking new VM root disk size is 11GB"
        lxc exec v1 -- df -B1000000000 | grep sda2 | grep 11

        echo "===> Renaming VM"
        lxc stop -f v1
        lxc rename v1 v1renamed

        echo "==> Deleting VM, vmbig image and vmsmall profile"
        lxc delete -f v1renamed
        lxc image delete vmbig
        lxc profile delete vmsmall

        echo "==> Deleting storage pool"
        lxc storage delete "${poolName}2"
        lxc storage delete "${poolName}"
done

FAIL=0
